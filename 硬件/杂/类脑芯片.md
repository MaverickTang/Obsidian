# 人工智能背景

## 芯片瓶颈

随着半导体行业进入了22纳米以下的工艺节点，虽然集成密度仍在持续提升，但是计算效果提升效率远不如前，具体原因包括：

1. 计算频率不再增加；

2. 晶体管的能量密度逐渐趋缓，导致计算能效逐渐趋缓；

3. 晶体管的成本在不同节点之间，几乎持平；在5nm/3nm最先进的工艺节点上，单位成本是处于爬升趋势。

## 两种架构

未来的10年是计算架构的黄金十年。他提出了特定领域架构设计思路：**DSA**。针对特定领域，进行可编程处理器的定制研发，从而获得计算加速，达到更好的性能

* 沿计算机科学发展而来AI加速器途径
* 神经形态计算途径

### AI加速器

主要是以高效支持当前的深度学习为主要架构优化目标，通过大规模的并行计算提升计算密度，从而获得性能提升，典型的代表包括谷歌的TPU、Graphcore的IPU、阿里的含光800等。

AI加速器的发展思路，可分为3个方面：

①计算密度，根据计算特点，开发并行计算能力更强的计算模块；

②通信带宽，采用带宽更高的通信模式；

③量身定制，通过计算权重的低精度化和稀疏计算等模式，节省计算开销。

### 神经形态架构

基于脑科学发展而来，通过模拟神经元和突触的典型特征，比如存算一体、脉冲编码、异步计算、动力学模型等特点，希望通过这些模拟从而达到更高的智能水平。其典型代表包括IBM TrueNorth,、Intel Loihi 和基于ARM架构发展而来的SpiNNaker

# 类脑计算趋势

## 深度学习与脑科学融合

深度学习和脑科学的发展有着各自的特点。一个是沿着计算机科学的导向，以精确的数值和矩阵计算为主，神经元处于连续值状态。另一个沿着脑模拟的方向发展而来，以0/1脉冲序列表达信息流，所以也叫脉冲神经网络，编码里包含了时间信息；另外，神经元内部具有动力学特征，具有事件驱动、稀疏发放等特点。

![img](https://pic4.zhimg.com/80/v2-137692eef8cc676b5e45ea3e4e8165cf_1440w.webp)

在深度学习和脉冲神经网络融合的探索方面，清华大学类脑计算研究中心早在2014年就开始探索，于2019、2020和2021年连续发表了三篇Nature论文，分别从架构、软件编译和算法模型三个方面进行研究。首篇论文（Figure 16）讲述的是异构融合架构的类脑芯片设计，如何在架构层面支持ANN和SNN的融合。第二篇（Figure 17）讲述的是类脑编译软件系统架构，提出了类脑计算完备性。第三篇（Figure 18）在自然通讯发表的类脑算法文章，首次融合了全局学习和本地学习的融合算法，为类脑融合算法模型的训练和在线学习机制提供支撑。

## 应用

深度学习网络处理

脑科学研究

类脑芯片